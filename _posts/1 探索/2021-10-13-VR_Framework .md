---
layout: post
title: VR 基本框架
categories: VR
---

> 参考 [聊一聊VR虚拟现实(二十一)](https://zhuanlan.zhihu.com/p/358555105) [聊一聊VR虚拟现实(十三)](https://zhuanlan.zhihu.com/p/101307076) [聊一聊VR虚拟现实(一)](https://www.zhihu.com/column/deerhunt) [聊一聊VR虚拟现实(四)](https://zhuanlan.zhihu.com/p/36959531)



### 基本历史

**VR不是VR眼镜，而是一种全新的范式**

- 2012年，**Oculus Rift** 问世，这是一款在 Kickstarter 上众筹250万美元的VR眼镜设备，创始人本身是一个VR收集控，他用遍了上个世纪的各种VR，体验都不尽人意，于是决定自己动手，于是便有了 Oculus Rift。
- 2014年，开源界的老大哥 Google 发布了其VR体验版解决方案：**CardBoard**。使得人们能以极低的价格，体验到新一代的VR的效果。
- 2015年，**HTC Vive** 在 MWC2015 上正式发布，次年索尼公布 **PSVR**，随后大量的厂家开始研发自己的VR设备，VR新元年正式开始。



### 基本架构

**Hardware** GPU-Docker-Cloud-5G-GaaS-IoTOS-CV/Interaction→ **VR** ←DCC-Tools/Platform-DigtalTwins-CG/CV/ML **Software**

#### 基础硬件架构

*处理器 + 显示器 + 光学系统 + 定位与姿态检测设备*

- **处理器**：计算核心。用来本地计算生成图像、根据陀螺仪和识别仪器的数据计算姿态定位、联网传输计算以及与物联系统连接协同计算。**为防止眩晕要求图像刷新率达到90Hz、丰富数字内容要求快速而低延迟的高分辨率图形与实时复杂渲染图形、实时的神经网络计算、实时高速网络传输与互联协同计算**，对运算速度要求很高。所以VR的处理器芯片性能指标至关重要。

- **显示器**：分别向左右眼睛显示图像。一般当我们说2k屏幕的VR眼镜时，是指一整块屏幕的**长边尺寸**，比如2k*1k尺寸。但如果说单眼2k，则是指屏幕**短边尺寸**是2k。2017年市场上主流的配置是单眼1.5k左右。屏幕分辨率越高，要求配备的处理器也越强大。

- **光学系统**：如果把显示器直接贴在人眼前，人眼是很难聚焦这么近的物体。凸透镜片的目的，就是通过折射光线，将显示器上的画面成像拉近到视网膜位置，使人的眼睛能轻松看清几乎贴在眼前的显示屏。目前还有**近眼增强现实**等新光学系统。

- **姿态检测**：显示器里的景象如果要随着人头部的运动而实时产生变化。则必须知道人头部的朝向，比如当带着VR眼镜的人向上看时，眼睛里的显示器需要实时的显示虚拟的天空，而这个“向上看”的动作，就需要**陀螺仪**来检测。**手势识别、身体姿态识别、表情识别、眼动追踪**。手势识别有两个技术路线：一是需要佩戴手套，靠硬件来识别，二是基于**计算机视觉**来识别。前者优势是精准，后者的优势是使用简便。**眼动追踪**则主要是**计算机视觉**方案，现在也有眼部肌电信号来进行眼动追踪的方法，但是只存在于实验室里。最后**表情识别**只能靠**计算机视觉**来做。以上提到的所有的识别方面的内容，都还处于比较前沿的状态，在实际应用中主要还是B端场景，C端使用的相对较少。Oculus Quest 上近期刚推出了手势识别的接口，预计后面会有越来越多的C端应用尝试这种新的交互方式。

- **定位设备**

- **外置激光发射器定位**（Lighthouse，由Valve公司研制，HTC-vive）：通过计算头盔和手柄接收到激光信号的时间差，推导出设备的空间坐标。特点：速度快，位置准。缺点：成本高。
- **外置视觉定位** 通过外部放置摄像头，拍摄头盔/手柄上的光点，来推算出设备的位置，Oculus Rift（红外线）和 PSVR（可见光）都是使用这种方式。想要准确高效地检测出光标点，就需要**图像处理、计算机视觉**的知识。
- **内置视觉定位**（InsideOut定位）通过头盔上的摄像头拍摄画面的变化，来估计头盔运动。微软WMR、Quest使用的是这种方式。优势是不需要额外架设设备。但是定位精度上比激光定位要差一些。为了能根据画面来推断相机的运动，也需要**计算机视觉**相关知识。InsideOut 头部定位对应的手部定位稍微复杂点，它又分为电磁手柄定位、超声手柄定位和视觉手柄定位三种方式。前两种一般是硬件直接给出定位坐标，最后一种仍然是基于**计算机视觉**，只不过摄像头从外置摄像头换成了头盔上的摄像头。


- 

<img src="https://pic1.zhimg.com/v2-de05e5887a577ec8e87876434f0f18fc_b.jpg" />

另外，这里还要再介绍两个概念：**3dof** 和 **6dof**。

**dof：degree of freedom**，即自由度。VR中，3dof是指3个转动角度的自由度。而6dof是指，除了3个转动角度外，再加上 位置相关的3个自由度（上下、左右、前后）。当我们说 3dof VR 时，是指该VR眼镜可以检测到固定头部的3种转动，但是不能检测到头部的上下前后左右的移动。而 6dof 眼镜，则是可以全面的检测到头部的空间和角度信息。

#### 分类

- **手机盒子**：嵌入机 ，Screenless Viewer
- **VR头戴显示器**：PCVR，Tethered HMD
- **一体机**：Standalone HMD, All in one HMD

##### 手机盒子

属于体验级VR眼镜产品，基本都是 3dof VR。它利用用户的手机，担任了处理器 + 显示器 + 陀螺仪的角色，而VR眼镜本身只提供了一个凸透镜，这就使得这类眼镜的成本非常低。

<img src="https://pic3.zhimg.com/v2-2bd162f7af6401283233591950ee088e_b.jpg" />

市面上1000元以内的VR眼镜都是嵌入机，三星的Gear VR，小米VR，暴风魔镜，乐视VR，就属于此类。另外，一些高端的手机盒子还配带了3dof的手柄控制器，使得操作更加便利。如果要在手机盒子上实现6dof VR，则需要增加一些外设，比如 nolo 的 6dof 交互套件。另外一种实现手机盒子6dof VR的方式是，通过手机摄像头拍摄外部世界的画面，再实时计算画面的变化，来估计头部的位置变化。

##### 头戴显示器

相对于入门体验级的手机盒子，VR头戴显示器则属于VR眼镜里的高端存在。为了达到极优秀的显示效果，它们需要连接PC（Sony的PSVR是连接PS4），使用PC的CPU和显卡来进行运算。

<img src="https://pic3.zhimg.com/v2-6e3166438e322adb7e749e8e70985f72_b.jpg" />

另外，跟手机盒子比，它们自带 6dof 头部检测 和 6dof 手柄。当前的VR头戴显示器，比较好的是四大厂商的产品：

- HTC vive
- Oculus
- PSVR
- 微软MR头盔

这四种眼镜最大的区别在于其 6odf 定位技术的不同。目前的 VR 6dof检测技术主要有三类：

1. **外置激光定位** 通过外置的激光发射器对设备进行定位，特点是速度快，位置准，缺点是**成本高**。**HTC vive**便是用了此种方式（**Lighthouse** 技术，由 **Valve** 公司研制）。
2. **外置图像处理定位** 通过外部放置摄像头，拍摄头盔/手柄上的光点来进行定位，**Oculus**（红外线） 和 **PSVR**（可见光）都是使用这种方式。
3. **内置图像处理定位**（**InsideOut** 定位）通过头盔上的摄像头拍摄画面的变化，来估计头盔运动。**微软MR眼镜**使用的是这种方式。它的优势是不需要额外架设设备。但是定位精度上，比激光定位要差一些。头戴显示器，制作成本比手机盒子要高很多。另外使用的时候还需要配合功能强大的PC或者PS4(PSVR)。所以一整套设备下来，**总体价位很高**。但是它的**体验确实是最好的**。

##### 一体机

<img src="https://pic3.zhimg.com/v2-84e2257c4bf9114f513150e19242a7d2_b.jpg" />

目前大部分一体机还都是头部 3dof + 手柄 3dof，价格介于手机盒子和头盔显示器之间，下一代会有头部 6dof 和 手柄 6dof 的一体机，比如 **Pico 的 Neo CV**，使用摄像头进行 InsideOut 定位。一体机最大的优势就是**便携**，头盔显示器虽然显示性能好，但是无法随身携带（比如外出旅行）。另外，即便都是在房间中使用，头盔显示器的连线，也会一定程度上阻碍用户的自由移动。

这三大类 VR 眼镜，性能、价位、应用场景各不相同，手机盒子方式，因为其体验较差（分辨率低、传感器差），从2018年期逐渐开始被淘汰，一般存在于一些线下快速体验的场景中（为了省钱）。而一体机在最近两年，开始逐渐崛起，在VR设备中占据了半壁江山。



### 主要问题

#### 清晰度/分辨率

##### 视场角/PDD/单目与双目

**视网膜屏**（裸眼分辨不出像素颗粒）。乔布斯把视网膜屏定义为：**300 PPI（Pixel Per Inch）**。也就是每 inch 300 个像素。而这个分辨率的屏幕，在VR眼镜里却远远不够用，因为VR眼镜相当于用一个放大镜看屏幕，原来肉眼看不到的像素点，当然就现了原形。

VR眼镜如果要达到视网膜屏需要多高的单位分辨率？这里先引入一个更加跨平台通用的概念：**PPD（Pixel Per Degree）**，即**每视场角像素数**。目前业内比较流行的说法是 60 PPD 是视网膜屏。也就是说，当你视野里每度里面有60个像素以上时，你就分辨不出像素颗粒了。IPhone4的PDD是64像素。

如果按照 **60 PPD** 的标准，一个 **视场角（FOV）100度** 的VR眼镜，单眼就需要 100 x 60 = 6000 个像素。也就是**单目6k，双目12k（现在VR眼镜一般标称都是双目的分辨率）**。那么我们现在主流的VR眼镜，分辨率是多少呢？

<img src="https://pic3.zhimg.com/v2-2418529f4875c39743562282e8805f0a_b.jpg" />

在最早期，三大VR眼镜普遍使用 **2k** 的分辨率，这个分辨率下就会有比较明显的**纱窗效应**。而目前，双目 **3k** 屏（2880x1600）是市场上已有的主流顶级分辨率，这个分辨率下，纱窗效应已经并不明显。

由此可见，目前能成熟应用的屏幕分辨率远远达不到 12k 的要求。那是不是清晰度这件事儿就遥遥无期了呢？

其实并不是。这里面的问题在于最开始提到的 60 PPD 这个业界流行标准。这个标准在我看来有点过于夸大。如前面所说，60 PPD 相当于把手机放到40厘米处的情况。但是实际实验中我们发现，即便是把手机放到20厘米处时，肉眼仍然很难看到像素颗粒，尤其是在看图片时。而20厘米处观察手机屏幕，相当于只有**30度左右的PPD**，按照这个新的标准，100度视场角的VR眼镜，**单目**需要3000多个像素可以达到有实用价值的视网膜屏，也就是**双目6k**。这个数值，跟目前市场上已经存在的 **4k** 比，其实差别就没那么大了。

#### 计算性能/性能优化

##### 视频分片解码

屏幕问题解决了，随之而来的便是为能充分利用视网膜屏，而需要的计算能力。

我们知道，VR视频是可以360度观看的（也有180度视频），对于一个 4k 360度2D全景视频来说，它的长是4k，高是2k，这个 4k 的边长，需要最终被环绕到一个360度的圆上。当我们使用一个100度视场角的VR眼镜来观看时，人眼在一个方向上能看到的视频提供的像素数是 1111 。

但对于一个**单目20度左右的PPD，总视场角（FOV）100度，双目4k**的屏来说，单目的可用像素数达到 2000 个。

*其关键在于视频PPD和单目PPD的比例。屏幕的PPD是20，而4k/360的PPD是11。如果屏幕PPD为30，FOV120，即接近双目8K，单目PPD是视频PPD的三倍。*

所以说在一个该种4k屏VR眼镜上播放4k视频，是远远不够的，4k屏应该播放12k视频。

<img src="https://pic4.zhimg.com/v2-bc0f51f6a0f13e6bea9f097a3e28abcb_b.jpg" />

但是问题是，8k 视频，或者 12k 视频，解码难度是相当高的。拿现在的一体机处理芯片来说，顶级的骁龙835芯片只能流畅的解码 4k 视频。离 8k、12k 还很远很远。好在我们有比较巧妙的解决方案避开这个问题，那就是**分片解码**。分片解码的概念其实很简单直观，那就是：看哪儿解哪儿。同时只能看到100度？那就只解100度好了。这样8k视频只需要解码2k左右，当前的移动处理芯片完全没问题。

<img src="https://pic2.zhimg.com/v2-ebe876a7d1aaac065842d6452d386ab1_b.jpg" />

##### 定位识别/注视点渲染

视频说完了，我们再说一下VR游戏和应用需要用到的3D渲染。随着分辨率的提升，渲染一个完整的场景，需要的计算力也是越来越大，尤其是场景还有非常复杂的材质、模型、光影特效时。那么这个时候就需要另一个技术来降低处理难度，即：**注视点渲染技术**。注视点渲染技术利用到了人眼的一个特性：虽然人眼的整体FOV很大，但是同一时间内，人只能分辨清很小视场角内的景物。根据我们的实验，当把**视野中央直径40度**以外的图像压缩到**1/4分辨率**时，人眼根本不会感知到这个分辨率的下降。也就是说，对于一个100度视场角的VR眼镜，我们可以只对40度范围内的景物进行高清渲染，对于40度以外的部分，只需要用1/4分辨率来渲染，这样就可以大大降低渲染的计算力需求。

当然，人的眼睛并不会只盯着正前方看，所以注视点渲染技术还需要结合**眼球追踪**技术来使用。眼球追踪技术目前已经有比较成熟的方案，但是**特定区域渲染**这个技术目前还没有推广开，需要各大厂商共同推进。

##### 云渲染与容器/GaaS

- 可参考微软模拟飞行。部分内容实时云端渲染传输，以提高算力可能。
- 利用容器技术使得不同种类游戏即为灵活切换的不同的互联网服务。

#### 数字内容缺口/接口

##### 数字内容缺口/CG-CV

一个新的计算平台的普及，需要丰富的内容生态。当前，移动VR平台软件游戏量是1000多款，PCVR 软件游戏量是 2000多款，PSVR 软件游戏量是 200多款。这几个数字，跟PC和手机上的软件和游戏量还远远不能比。目前比较影响软件内容增长的，主要有两个因素，首先是设备保有量低，导致对开发者吸引力低。当然，这个是行业发展阶段决定的。只能说希望大的游戏厂商能砸钱来推动下市场。

**数字孪生**。现实中有无数培训内容，如果要全面虚拟化，需要大量工程技术人员投入把现实内容复刻到VR中去。可能，未来实体产品出厂时会对外提供标准3D模型供培训公司使用。这个市场是巨大的。

##### 平台接口

另一个因素，则是VR行业的**软件标准**不统一问题。尤其是移动VR平台，完全的碎片化，虽然都是安卓平台，但是每家的SDK标准都不同。软件开发商需要针对不同的眼镜平台，分别对接不同的SDK，分别调试，这对开发者来说都是工作量。好在现在有了**OpenXR**标准组织，力图把各个平台的接口标准统一，这样开发者只要开发一次应用，就能在不同平台上分布运行。这对VR软件行业是一大利好。

<img src="https://pic3.zhimg.com/v2-0a7eae7fbe26dc39e2767d799e437f4a_b.jpg" />
<img src="https://pic2.zhimg.com/v2-a25d9cd1b2e3ca71b010f3eda6f66839_b.jpg" />

#### 软硬协同/对VR优化

##### 内容设计

对于VR的眩晕问题，很多人有个误区，那就是提及眩晕，必谈“晕动症”。其实晕动症只是VR眩晕中的一个诱因之一。而且目前很多VR应用和游戏，都会想办法避开**非同步运动**这个问题点。也就是说，目前很多**游戏内容设计**从**操作方式**上都不会面临这个问题。

再说回到“晕动症”，它也不是人类的一个固有的生理反应，不是说只要虚拟运动与实际运动不符就一定会晕，而是与人的**意识认知**紧密相关。这里有几个实验可以说明一些问题：当人在虚拟向前运动时，快速摆动胳膊，眩晕感会降低。当人在虚拟拐弯时，如果身体同时向一侧倾斜，眩晕感会降低。最有趣的一个现象，当属运动游戏《Sprint vector》，在里面玩家要通过向后“拖拽”整个场景的方式来向前移动。神奇的是即便是人在现实中的原地不动，而游戏内的场景高速向后移动时，也感受不到眩晕感（笔者亲自体验）。虽然“拖拽”的方式在游戏中适应性并不广，但是它至少提供了一个方向，即：人类对虚拟世界运动的**认知**，会极大的影响眩晕感的强弱。很有可能未来的“晕动症”，会在**脑科学**和**心理学**的夹击下被彻底消除，届时人类就可以真正的在虚拟世界中**随心所欲的穿行**。

##### 软硬协同设计与性能优化

其实除了晕动症，还会有很多其他问题会造成用户眩晕，包括但不限于：**屏幕刷新率（软硬性能）、光学畸变、显示延迟（软硬性能）、定位准确度（软硬性能）** 等。当前市场中，用户遇到的眩晕问题，更多是由这类因素造成的。好在这些都是“可解”的。也就是说，只要厂商悉心去**调节**这块，就可以将眩晕问题搞定。**高性能渲染管线**。

当引擎渲染出画面后，反畸变、合成、位置预测等过程到屏幕上。为解决VAC（辐辏冲突）问题，未来还要引入光场显示技术。**计算机图形学**相关知识。

##### 识别与定位/3DCV

内置视觉定位（InsideOut定位）通过头盔上的摄像头拍摄画面的变化，来估计头盔运动。微软WMR、Quest使用的是这种方式。优势是不需要额外架设设备。但是定位精度上比激光定位要差一些。为了能根据画面来推断相机的运动，也需要计算机视觉相关知识。InsideOut 头部定位对应的手部定位稍微复杂点，它又分为电磁手柄定位、超声手柄定位和视觉手柄定位三种方式。前两种一般是硬件直接给出定位坐标，最后一种仍然是基于**计算机视觉**，只不过摄像头从外置摄像头换成了头盔上的摄像头。

**手势识别、身体姿态识别、表情识别、眼动追踪**。手势识别有两个技术路线：一是需要佩戴手套，靠硬件来识别，二是基于计算机视觉来识别。前者优势是精准，后者的优势是使用简便。眼动追踪则主要是 计算机视觉 方案，现在也有眼部肌电信号来进行眼动追踪的方法，但是只存在于实验室里。最后，表情识别只能靠 计算机视觉 来做。以上提到的所有的识别方面的内容，都还处于比较前沿的状态，在实际应用中主要还是B端场景，C端使用的相对较少。Oculus Quest 上近期刚推出了手势识别的接口，预计后面会有越来越多的C端应用尝试这种新的交互方式。

#### 大规模全真物联网

##### IoTVR/物联化虚拟

多平台协同操作的 平滑衔接的 接入云的 移动设备的 微内核操作系统。

表情检测技术还停留在实验室阶段（但眼动检测已用在很多B端场景）。预计未来一两年在C端落地。

##### 全真互联网/大数据同步传输

另外，未来将大行其道的虚拟社交（全真互联网），对网络传输要求也会比较高，形象同步需要传输的数据量跟传统网游比大了一个量级。千人同服，还需要在数据同步方面做很多研究。

##### 区块链经济系统

- **区块链**。因为虚拟世界中的**经济系统**可以跟现实世界打通，虚拟世界中会有新的经济体出现，你在虚拟世界中，可以得到一份真正可以养家糊口的工作。比如，虚拟世界场景建筑师，或者虚拟世界中的一个互动演员。